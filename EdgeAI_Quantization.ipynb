{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy35bNKD7pic"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from datasets import load_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcM7AwtJ7pWF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"gpt2\"  # Replace with your LLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V7anVeb7pSg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QuantizedModelPipeline:\n",
        "    def __init__(self, model_name, quantization_config=None, max_tokens=64):\n",
        "        self.model_name = model_name\n",
        "        self.quantization_config = quantization_config\n",
        "        self.max_tokens = max_tokens\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load_quantized_model(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            quantization_config=self.quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        self.model.eval()\n",
        "        print(f\"Loaded model: {self.model_name} with 4-bit quantization\")\n",
        "\n",
        "    def calculate_perplexity(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            start_time = time.time()\n",
        "            outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            end_time = time.time()\n",
        "        loss = outputs.loss.item()\n",
        "        return np.exp(loss), end_time - start_time\n",
        "\n",
        "    def generate_and_evaluate_bleu(self, input_text, reference_text):\n",
        "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "        start_time = time.time()\n",
        "        outputs = self.model.generate(inputs, max_new_tokens=self.max_tokens)\n",
        "        end_time = time.time()\n",
        "\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        smoothie = SmoothingFunction().method4\n",
        "        reference = [reference_text.split()]\n",
        "        candidate = generated_text.split()\n",
        "        bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "\n",
        "        return generated_text, bleu_score, end_time - start_time\n",
        "\n",
        "    def evaluate_sample(self, input_text, reference_text):\n",
        "        ppl, ppl_time = self.calculate_perplexity(input_text)\n",
        "        generated_text, bleu, gen_time = self.generate_and_evaluate_bleu(input_text, reference_text)\n",
        "\n",
        "        return {\n",
        "            \"generated\": generated_text,\n",
        "            \"perplexity\": ppl,\n",
        "            \"perplexity_time\": ppl_time,\n",
        "            \"bleu_score\": bleu,\n",
        "            \"generation_time\": gen_time\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHthvu3-7pPm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define quant config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = QuantizedModelPipeline(\"gpt2\", quantization_config=bnb_config)\n",
        "pipeline.load_quantized_model()\n",
        "\n",
        "# Load a dataset sample\n",
        "dataset = load_dataset(\"xsum\", split=\"validation[:5]\")\n",
        "sample = dataset[0]\n",
        "input_text = sample[\"document\"]\n",
        "reference_text = sample[\"summary\"]\n",
        "\n",
        "# Evaluate\n",
        "result = pipeline.evaluate_sample(input_text, reference_text)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtTu7qh-7pNG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkoRFgGP7pKf"
      },
      "outputs": [],
      "source": [
        "%cd llama.cpp\n",
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6utE_WX7pHv"
      },
      "outputs": [],
      "source": [
        "%cd .\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/AdamLucek/Orpo-Llama-3.2-1B-15k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8YnI6AI7pEi"
      },
      "outputs": [],
      "source": [
        "# convert the model to ggml FP16 format\n",
        "!python3 llama.cpp/convert_hf_to_gguf.py Orpo-Llama-3.2-1B-15k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AakTIAA67pAx"
      },
      "outputs": [],
      "source": [
        "# quantize the model to 4-bits (using Q4_K_M method)\n",
        "!llama.cpp/llama-quantize Orpo-Llama-3.2-1B-15k/Llama-3.2-1B-15k-F16.gguf ./Orpo-Llama-3.2-1B-15k/Orpo-Llama-3.2-1B-15k-Q4_K_M.gguf Q4_K_M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTvsRZ9e7o90"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYwz-qz17o7S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

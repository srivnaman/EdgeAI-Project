{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd0e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMLayerImportance:\n",
    "    def __init__(self, model_name=\"gpt2\", device=None):\n",
    "        \"\"\"\n",
    "        Initialize the LLMLayerImportance class to analyze layer importance in LLMs.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_name : str\n",
    "            The name of the pretrained model to load (e.g., \"gpt2\", \"llama-7b\", \"mistral-7b\")\n",
    "        device : torch.device or None\n",
    "            Device to load the model on. If None, uses CUDA if available, otherwise CPU.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "        # Get model architecture information\n",
    "        self.num_layers = self.model.config.num_hidden_layers\n",
    "        self.num_attention_heads = self.model.config.num_attention_heads\n",
    "        \n",
    "        # Store reference to whether model uses multi-head attention\n",
    "        if hasattr(self.model.config, \"is_decoder\") and self.model.config.is_decoder:\n",
    "            self.has_cross_attention = True\n",
    "        else:\n",
    "            self.has_cross_attention = False\n",
    "            \n",
    "        # Initialize layer mask attributes\n",
    "        self.layer_masks = None\n",
    "        \n",
    "    def get_dataloader(self, path, name=None, split=\"validation\", batch_size=8, shuffle=False):\n",
    "        \"\"\"\n",
    "        Create a dataloader for evaluating layer importance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            Dataset path or name from HuggingFace datasets\n",
    "        name : str or None\n",
    "            Specific dataset configuration name\n",
    "        split : str\n",
    "            Dataset split to use (e.g., \"train\", \"validation\", \"test\")\n",
    "        batch_size : int\n",
    "            Batch size for dataloader\n",
    "        shuffle : bool\n",
    "            Whether to shuffle the dataset\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.utils.data.DataLoader\n",
    "            DataLoader containing preprocessed dataset samples\n",
    "        \"\"\"\n",
    "        from datasets import load_dataset\n",
    "        from torch.utils.data import DataLoader\n",
    "        \n",
    "        dataset = load_dataset(path, name, split=split)\n",
    "        dataset = self._preprocess_dataset(path, dataset)\n",
    "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    def _preprocess_dataset(self, path, dataset):\n",
    "        \"\"\"\n",
    "        Preprocess different datasets based on their type.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            Dataset path or name\n",
    "        dataset : datasets.Dataset\n",
    "            Dataset object to preprocess\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        datasets.Dataset\n",
    "            Preprocessed dataset\n",
    "        \"\"\"\n",
    "        if path == \"wikitext\":\n",
    "            return self._preprocess_wikitext(dataset)\n",
    "        elif path == \"glue\":\n",
    "            return self._preprocess_glue(dataset)\n",
    "        elif path == \"openai/webtext\":\n",
    "            return self._preprocess_webtext(dataset)\n",
    "        elif path == \"imdb\":\n",
    "            return self._preprocess_imdb(dataset)\n",
    "        else:\n",
    "            raise ValueError(f\"Preprocessing for dataset {path} is not implemented.\")\n",
    "            \n",
    "    def _preprocess_wikitext(self, dataset):\n",
    "        \"\"\"Preprocess WikiText dataset for language modeling\"\"\"\n",
    "        def preprocess(batch):\n",
    "            inputs = self.tokenizer(batch[\"text\"], padding=\"max_length\", \n",
    "                                    truncation=True, max_length=512)\n",
    "            inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "            return inputs\n",
    "            \n",
    "        return dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    def _preprocess_glue(self, dataset):\n",
    "        \"\"\"Preprocess GLUE dataset for classification\"\"\"\n",
    "        def preprocess(batch):\n",
    "            if \"sentence\" in batch:\n",
    "                texts = batch[\"sentence\"]\n",
    "            elif \"sentence1\" in batch and \"sentence2\" in batch:\n",
    "                texts = [s1 + \" [SEP] \" + s2 for s1, s2 in zip(batch[\"sentence1\"], batch[\"sentence2\"])]\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported GLUE task format\")\n",
    "                \n",
    "            inputs = self.tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "            inputs[\"labels\"] = batch[\"label\"]\n",
    "            return inputs\n",
    "            \n",
    "        cols_to_remove = [col for col in dataset.column_names if col not in [\"label\"]]\n",
    "        return dataset.map(preprocess, batched=True, remove_columns=cols_to_remove)\n",
    "    \n",
    "    def _preprocess_webtext(self, dataset):\n",
    "        \"\"\"Preprocess WebText dataset for language modeling\"\"\"\n",
    "        def preprocess(batch):\n",
    "            inputs = self.tokenizer(batch[\"text\"], padding=\"max_length\", \n",
    "                                    truncation=True, max_length=512)\n",
    "            inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "            return inputs\n",
    "            \n",
    "        return dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    def _preprocess_imdb(self, dataset):\n",
    "        \"\"\"Preprocess IMDB dataset for sentiment classification\"\"\"\n",
    "        def preprocess(batch):\n",
    "            inputs = self.tokenizer(batch[\"text\"], padding=\"max_length\", \n",
    "                                   truncation=True, max_length=512)\n",
    "            inputs[\"labels\"] = batch[\"label\"]\n",
    "            return inputs\n",
    "            \n",
    "        return dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    def compute_layer_importance(self, dataloader, layer_mask=None, num_batches=50):\n",
    "        \"\"\"\n",
    "        Compute importance scores for layers in the LLM by measuring their influence on model predictions.\n",
    "        \n",
    "        Layer importance quantifies how much each layer contributes to the model's performance/loss.\n",
    "        Higher importance values indicate layers that have greater impact on the model's predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataloader : torch.utils.data.DataLoader\n",
    "            DataLoader containing evaluation data\n",
    "        layer_mask : torch.Tensor or None\n",
    "            Optional initial mask for model layers of shape (num_layers,)\n",
    "            Contains 0s and 1s, where 1 indicates the layer is active\n",
    "        num_batches : int\n",
    "            Number of batches to use for importance computation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Normalized importance scores for each layer of shape (num_layers,)\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize layer masks if not provided\n",
    "        if layer_mask is None:\n",
    "            layer_mask = torch.ones(self.num_layers, device=self.device, requires_grad=True)\n",
    "        else:\n",
    "            # Ensure mask has requires_grad=True for gradient computation\n",
    "            layer_mask = layer_mask.clone().detach().requires_grad_(True)\n",
    "            \n",
    "        # Store the mask for potential later use\n",
    "        self.layer_masks = layer_mask\n",
    "        \n",
    "        # Initialize importance scores\n",
    "        layer_importance = torch.zeros(self.num_layers, device=self.device)\n",
    "        \n",
    "        # Register hooks for each transformer layer to apply masks\n",
    "        hooks = []\n",
    "        \n",
    "        def get_layer_hook(layer_idx):\n",
    "            def hook(module, input, output):\n",
    "                # Apply layer mask through scaling\n",
    "                mask_value = layer_mask[layer_idx]\n",
    "                # Scale the output by the mask value\n",
    "                return output * mask_value\n",
    "            return hook\n",
    "        \n",
    "        # Attach hooks to transformer layers\n",
    "        for i in range(self.num_layers):\n",
    "            if hasattr(self.model, \"transformer\"):\n",
    "                # GPT-2 style models\n",
    "                layer = self.model.transformer.h[i]\n",
    "            elif hasattr(self.model, \"model\"):\n",
    "                # Some models have nested structure\n",
    "                if hasattr(self.model.model, \"layers\"):\n",
    "                    layer = self.model.model.layers[i]\n",
    "                else:\n",
    "                    layer = self.model.model.decoder.layers[i]\n",
    "            elif hasattr(self.model, \"decoder\"):\n",
    "                # Decoder-only models\n",
    "                layer = self.model.decoder.layers[i]\n",
    "            elif hasattr(self.model, \"layers\"):\n",
    "                # Direct layers attribute\n",
    "                layer = self.model.layers[i]\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model architecture: {type(self.model)}\")\n",
    "                \n",
    "            hook = layer.register_forward_hook(get_layer_hook(i))\n",
    "            hooks.append(hook)\n",
    "        \n",
    "        # Process batches\n",
    "        batch_count = 0\n",
    "        for batch in dataloader:\n",
    "            if batch_count >= num_batches:\n",
    "                break\n",
    "                \n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device) if \"attention_mask\" in batch else None\n",
    "            labels = batch[\"labels\"].to(self.device)\n",
    "            \n",
    "            # Reset gradients\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            # Forward pass with layer masking applied through hooks\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Accumulate gradients (absolute values) for importance scores\n",
    "            layer_importance += torch.abs(layer_mask.grad.detach())\n",
    "            \n",
    "            # Reset the gradient for the next iteration\n",
    "            layer_mask.grad.zero_()\n",
    "            \n",
    "            batch_count += 1\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Normalize importance scores to [0, 1]\n",
    "        max_importance = layer_importance.max()\n",
    "        if max_importance > 0:\n",
    "            layer_importance = layer_importance / max_importance\n",
    "        \n",
    "        return layer_importance\n",
    "    \n",
    "    def visualize_layer_importance(self, layer_importance, save_path=\"layer_importance.png\"):\n",
    "        \"\"\"\n",
    "        Visualize layer importance scores as a bar chart.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer_importance : torch.Tensor\n",
    "            Tensor of layer importance scores from compute_layer_importance\n",
    "        save_path : str\n",
    "            Path to save the visualization\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        import numpy as np\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Convert to numpy for plotting\n",
    "        importance_np = layer_importance.cpu().numpy()\n",
    "        \n",
    "        # Create bar plot with layer indices\n",
    "        sns.barplot(x=np.arange(self.num_layers), y=importance_np)\n",
    "        \n",
    "        plt.title(f\"Layer Importance Scores for {self.model.config.model_type}\")\n",
    "        plt.xlabel(\"Layer Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.xticks(np.arange(0, self.num_layers, max(1, self.num_layers // 10)))\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add importance values as text above bars\n",
    "        for i, val in enumerate(importance_np):\n",
    "            if i % max(1, self.num_layers // 10) == 0:  # Only show some values to avoid clutter\n",
    "                plt.text(i, val + 0.02, f\"{val:.2f}\", ha='center', va='bottom', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        \n",
    "    def prune_layers(self, layer_importance, pruning_ratio=0.3):\n",
    "        \"\"\"\n",
    "        Create a pruning mask based on layer importance scores.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer_importance : torch.Tensor\n",
    "            Tensor of layer importance scores\n",
    "        pruning_ratio : float\n",
    "            Ratio of layers to prune (0.0 to 1.0)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Binary mask indicating which layers to keep (1) and which to prune (0)\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Determine number of layers to prune\n",
    "        num_to_prune = int(self.num_layers * pruning_ratio)\n",
    "        \n",
    "        # Create full mask (all 1s initially)\n",
    "        pruning_mask = torch.ones_like(layer_importance)\n",
    "        \n",
    "        if num_to_prune > 0:\n",
    "            # Get indices of least important layers\n",
    "            _, indices = torch.topk(layer_importance, self.num_layers - num_to_prune, largest=True)\n",
    "            \n",
    "            # Create mask (0 for pruned layers, 1 for kept layers)\n",
    "            pruning_mask = torch.zeros_like(layer_importance)\n",
    "            pruning_mask[indices] = 1.0\n",
    "            \n",
    "        return pruning_mask\n",
    "        \n",
    "    def evaluate_with_mask(self, dataloader, layer_mask, num_eval_batches=50):\n",
    "        \"\"\"\n",
    "        Evaluate model performance with a given layer mask.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataloader : torch.utils.data.DataLoader\n",
    "            DataLoader containing evaluation data\n",
    "        layer_mask : torch.Tensor\n",
    "            Binary mask indicating which layers to use\n",
    "        num_eval_batches : int\n",
    "            Number of batches to use for evaluation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average loss on the evaluation data\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Register hooks for each transformer layer to apply masks\n",
    "        hooks = []\n",
    "        \n",
    "        def get_layer_hook(layer_idx):\n",
    "            def hook(module, input, output):\n",
    "                # Apply layer mask - if mask is 0, effectively disables the layer\n",
    "                mask_value = layer_mask[layer_idx]\n",
    "                if mask_value == 0:\n",
    "                    # For pruned layer, pass through the input directly\n",
    "                    return input[0]  # Most transformers return a tuple where first element is the layer input\n",
    "                else:\n",
    "                    return output\n",
    "            return hook\n",
    "        \n",
    "        # Attach hooks to transformer layers\n",
    "        for i in range(self.num_layers):\n",
    "            if hasattr(self.model, \"transformer\"):\n",
    "                layer = self.model.transformer.h[i]\n",
    "            elif hasattr(self.model, \"model\"):\n",
    "                if hasattr(self.model.model, \"layers\"):\n",
    "                    layer = self.model.model.layers[i]\n",
    "                else:\n",
    "                    layer = self.model.model.decoder.layers[i]\n",
    "            elif hasattr(self.model, \"decoder\"):\n",
    "                layer = self.model.decoder.layers[i]\n",
    "            elif hasattr(self.model, \"layers\"):\n",
    "                layer = self.model.layers[i]\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model architecture: {type(self.model)}\")\n",
    "                \n",
    "            hook = layer.register_forward_hook(get_layer_hook(i))\n",
    "            hooks.append(hook)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                if batch_count >= num_eval_batches:\n",
    "                    break\n",
    "                    \n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device) if \"attention_mask\" in batch else None\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                total_loss += outputs.loss.item()\n",
    "                batch_count += 1\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        return total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "    \n",
    "    def find_optimal_pruning(self, dataloader, importance_scores, pruning_ratios=None, tolerance=0.1):\n",
    "        \"\"\"\n",
    "        Find the optimal pruning ratio by testing different ratios.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataloader : torch.utils.data.DataLoader\n",
    "            DataLoader containing evaluation data\n",
    "        importance_scores : torch.Tensor\n",
    "            Layer importance scores\n",
    "        pruning_ratios : list or None\n",
    "            List of pruning ratios to try. If None, uses default values\n",
    "        tolerance : float\n",
    "            Maximum acceptable performance degradation ratio\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (optimal_pruning_ratio, optimal_mask, performance_results)\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        if pruning_ratios is None:\n",
    "            pruning_ratios = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        \n",
    "        # First evaluate the model with no pruning to get baseline performance\n",
    "        baseline_mask = torch.ones(self.num_layers, device=self.device)\n",
    "        baseline_loss = self.evaluate_with_mask(dataloader, baseline_mask)\n",
    "        \n",
    "        results = []\n",
    "        optimal_ratio = 0.0\n",
    "        optimal_mask = baseline_mask\n",
    "        \n",
    "        for ratio in pruning_ratios:\n",
    "            # Generate mask based on importance scores\n",
    "            mask = self.prune_layers(importance_scores, ratio)\n",
    "            \n",
    "            # Evaluate with this mask\n",
    "            loss = self.evaluate_with_mask(dataloader, mask)\n",
    "            \n",
    "            # Calculate relative performance degradation\n",
    "            relative_degradation = (loss - baseline_loss) / baseline_loss\n",
    "            \n",
    "            results.append({\n",
    "                'pruning_ratio': ratio,\n",
    "                'loss': loss,\n",
    "                'relative_degradation': relative_degradation,\n",
    "                'mask': mask.clone()\n",
    "            })\n",
    "            \n",
    "            # Update optimal if within tolerance and most aggressive pruning so far\n",
    "            if relative_degradation <= tolerance and ratio > optimal_ratio:\n",
    "                optimal_ratio = ratio\n",
    "                optimal_mask = mask.clone()\n",
    "        \n",
    "        return optimal_ratio, optimal_mask, results\n",
    "    \n",
    "    def visualize_pruning_results(self, results, save_path=\"pruning_results.png\"):\n",
    "        \"\"\"\n",
    "        Visualize the impact of different pruning ratios on model performance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        results : list\n",
    "            List of dictionaries with pruning results from find_optimal_pruning\n",
    "        save_path : str\n",
    "            Path to save the visualization\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        ratios = [r['pruning_ratio'] for r in results]\n",
    "        losses = [r['loss'] for r in results]\n",
    "        degradations = [r['relative_degradation'] for r in results]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "        \n",
    "        # Plot loss\n",
    "        ax1.plot(ratios, losses, 'o-', color='blue')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Effect of Layer Pruning on Model Performance')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot relative degradation\n",
    "        ax2.plot(ratios, degradations, 'o-', color='red')\n",
    "        ax2.axhline(y=0.1, color='green', linestyle='--', label='10% Degradation Threshold')\n",
    "        ax2.set_xlabel('Pruning Ratio')\n",
    "        ax2.set_ylabel('Relative Performance Degradation')\n",
    "        ax2.grid(True)\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    \n",
    "    def apply_permanent_pruning(self, layer_mask):\n",
    "        \"\"\"\n",
    "        Apply permanent pruning to the model by modifying its architecture.\n",
    "        \n",
    "        WARNING: This permanently modifies the model architecture and cannot be undone.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer_mask : torch.Tensor\n",
    "            Binary mask indicating which layers to keep (1) and which to prune (0)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        model\n",
    "            The pruned model\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        import copy\n",
    "        \n",
    "        # Create a copy of the model to avoid modifying the original\n",
    "        pruned_model = copy.deepcopy(self.model)\n",
    "        \n",
    "        # Get indices of layers to keep\n",
    "        keep_indices = torch.nonzero(layer_mask).squeeze().tolist()\n",
    "        \n",
    "        # Handle the case where only one layer is kept\n",
    "        if isinstance(keep_indices, int):\n",
    "            keep_indices = [keep_indices]\n",
    "        \n",
    "        # Modify the layer structure based on model type\n",
    "        if hasattr(pruned_model, \"transformer\"):\n",
    "            # GPT-2 style\n",
    "            pruned_model.transformer.h = torch.nn.ModuleList(\n",
    "                [pruned_model.transformer.h[i] for i in keep_indices]\n",
    "            )\n",
    "            pruned_model.config.num_hidden_layers = len(keep_indices)\n",
    "            \n",
    "        elif hasattr(pruned_model, \"model\"):\n",
    "            if hasattr(pruned_model.model, \"layers\"):\n",
    "                pruned_model.model.layers = torch.nn.ModuleList(\n",
    "                    [pruned_model.model.layers[i] for i in keep_indices]\n",
    "                )\n",
    "            else:\n",
    "                pruned_model.model.decoder.layers = torch.nn.ModuleList(\n",
    "                    [pruned_model.model.decoder.layers[i] for i in keep_indices]\n",
    "                )\n",
    "            pruned_model.config.num_hidden_layers = len(keep_indices)\n",
    "            \n",
    "        elif hasattr(pruned_model, \"decoder\"):\n",
    "            pruned_model.decoder.layers = torch.nn.ModuleList(\n",
    "                [pruned_model.decoder.layers[i] for i in keep_indices]\n",
    "            )\n",
    "            pruned_model.config.num_hidden_layers = len(keep_indices)\n",
    "            \n",
    "        elif hasattr(pruned_model, \"layers\"):\n",
    "            pruned_model.layers = torch.nn.ModuleList(\n",
    "                [pruned_model.layers[i] for i in keep_indices]\n",
    "            )\n",
    "            pruned_model.config.num_hidden_layers = len(keep_indices)\n",
    "            \n",
    "        # Update model attributes\n",
    "        self.model = pruned_model\n",
    "        self.num_layers = len(keep_indices)\n",
    "        \n",
    "        return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b695c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a921543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the LLMLayerImportance class we defined earlier\n",
    "# For this example, assume the class is in a file called llm_layer_importance.py\n",
    "# from llm_layer_importance import LLMLayerImportance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d2d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    print(\"Starting GPT-2 layer importance analysis...\")\n",
    "    \n",
    "    # Initialize the LLMLayerImportance with GPT-2 model\n",
    "    # Using the small GPT-2 model for faster computation\n",
    "    print(\"Loading GPT-2 model...\")\n",
    "    lli = LLMLayerImportance(model_name=\"gpt2\", device=None)  # None will use CUDA if available\n",
    "    \n",
    "    # Print model details\n",
    "    print(f\"Model: {lli.model.config.model_type}\")\n",
    "    print(f\"Number of layers: {lli.num_layers}\")\n",
    "    print(f\"Number of attention heads per layer: {lli.num_attention_heads}\")\n",
    "    \n",
    "    # Load and preprocess a small dataset for evaluation\n",
    "    # Using WikiText for language modeling evaluation\n",
    "    print(\"Loading WikiText dataset...\")\n",
    "    dataloader = lli.get_dataloader(\n",
    "        path=\"wikitext\",\n",
    "        name=\"wikitext-2-raw-v1\",\n",
    "        split=\"test\",\n",
    "        batch_size=4,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Compute layer importance\n",
    "    print(\"Computing layer importance scores...\")\n",
    "    layer_importance = lli.compute_layer_importance(\n",
    "        dataloader=dataloader,\n",
    "        num_batches=10  # Using a small number of batches for demonstration\n",
    "    )\n",
    "    \n",
    "    # Print the layer importance scores\n",
    "    print(\"\\nLayer Importance Scores:\")\n",
    "    for i, score in enumerate(layer_importance.cpu().numpy()):\n",
    "        print(f\"Layer {i}: {score:.4f}\")\n",
    "    \n",
    "    # Visualize the importance scores\n",
    "    print(\"\\nVisualizing layer importance...\")\n",
    "    lli.visualize_layer_importance(layer_importance, save_path=\"gpt2_layer_importance.png\")\n",
    "    print(\"Visualization saved to gpt2_layer_importance.png\")\n",
    "    \n",
    "    # Find optimal pruning ratio\n",
    "    print(\"\\nFinding optimal pruning configuration...\")\n",
    "    pruning_ratios = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    optimal_ratio, optimal_mask, results = lli.find_optimal_pruning(\n",
    "        dataloader=dataloader,\n",
    "        importance_scores=layer_importance,\n",
    "        pruning_ratios=pruning_ratios,\n",
    "        tolerance=0.1  # Allow up to 10% performance degradation\n",
    "    )\n",
    "    \n",
    "    # Visualize pruning results\n",
    "    lli.visualize_pruning_results(results, save_path=\"gpt2_pruning_results.png\")\n",
    "    print(\"Pruning results visualization saved to gpt2_pruning_results.png\")\n",
    "    \n",
    "    # Print optimal pruning information\n",
    "    print(f\"\\nOptimal pruning ratio: {optimal_ratio:.2f}\")\n",
    "    print(f\"Number of layers to keep: {int(lli.num_layers * (1 - optimal_ratio))}\")\n",
    "    print(f\"Layers to keep: {torch.nonzero(optimal_mask).squeeze().tolist()}\")\n",
    "    \n",
    "    # Evaluate the pruned model\n",
    "    print(\"\\nEvaluating model performance before and after pruning...\")\n",
    "    \n",
    "    # Baseline performance (no pruning)\n",
    "    baseline_mask = torch.ones(lli.num_layers, device=lli.device)\n",
    "    baseline_loss = lli.evaluate_with_mask(dataloader, baseline_mask, num_eval_batches=10)\n",
    "    print(f\"Original model loss: {baseline_loss:.4f}\")\n",
    "    \n",
    "    # Optimal pruning performance\n",
    "    pruned_loss = lli.evaluate_with_mask(dataloader, optimal_mask, num_eval_batches=10)\n",
    "    print(f\"Pruned model loss: {pruned_loss:.4f}\")\n",
    "    print(f\"Performance impact: {((pruned_loss - baseline_loss) / baseline_loss) * 100:.2f}%\")\n",
    "    \n",
    "    # Apply permanent pruning (optional)\n",
    "    if optimal_ratio > 0:\n",
    "        print(\"\\nDemonstrating permanent pruning...\")\n",
    "        original_num_layers = lli.num_layers\n",
    "        pruned_model = lli.apply_permanent_pruning(optimal_mask)\n",
    "        print(f\"Model permanently pruned from {original_num_layers} to {lli.num_layers} layers\")\n",
    "        \n",
    "        # Final evaluation of the permanently pruned model\n",
    "        final_dataloader = lli.get_dataloader(\n",
    "            path=\"wikitext\",\n",
    "            name=\"wikitext-2-raw-v1\",\n",
    "            split=\"test\",\n",
    "            batch_size=4,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        final_loss = lli.evaluate_with_mask(\n",
    "            final_dataloader, \n",
    "            torch.ones(lli.num_layers, device=lli.device),  # All layers active in pruned model\n",
    "            num_eval_batches=10\n",
    "        )\n",
    "        print(f\"Final pruned model loss: {final_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce02c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPT-2 layer importance analysis...\n",
      "Loading GPT-2 model...\n",
      "Model: gpt2\n",
      "Number of layers: 12\n",
      "Number of attention heads per layer: 12\n",
      "Loading WikiText dataset...\n",
      "Computing layer importance scores...\n",
      "Error occurred: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289296b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
